# Senri Model Configuration
# モデルアーキテクチャの設定

# Base model settings
base_model:
  name: "Qwen/Qwen2.5-0.5B"
  vocab_size: 151936

# Transformer architecture
architecture:
  hidden_size: 896
  intermediate_size: 4864
  num_hidden_layers: 24
  num_attention_heads: 14
  num_key_value_heads: 2  # GQA: 14 query heads, 2 KV heads
  head_dim: 64  # hidden_size / num_attention_heads

# Senri-specific settings
senri:
  # Sliding Window Attention
  sliding_window_size: 4096
  chunk_size: 64

  # Memory settings
  top_k_memories: 64

  # Memory layer configuration
  # Memory layers: [12, 16, 20]
  num_memory_layers: 3
  first_memory_layer: 12
  memory_layer_interval: 4

# Positional encoding
position_encoding:
  # SWA uses RoPE, Memory uses NoPE
  rope_theta: 1000000.0
  max_position_embeddings: 32768
