# Senri Model Configuration
# モデルアーキテクチャの設定

# Base model settings
base_model:
  name: "HuggingFaceTB/SmolLM-135M"
  vocab_size: 49152

# Transformer architecture (SmolLM-135M specs)
architecture:
  hidden_size: 576
  intermediate_size: 1536
  num_hidden_layers: 30
  num_attention_heads: 9
  num_key_value_heads: 3  # GQA: 9 query heads, 3 KV heads
  head_dim: 64  # hidden_size / num_attention_heads

# Senri-specific settings
senri:
  # Sliding Window Attention
  # Smaller window to force memory usage during training
  sliding_window_size: 1024
  chunk_size: 64

  # Memory settings
  top_k_memories: 64

  # Memory layer configuration
  # For 30-layer SmolLM: memory at layers 10 and 20
  # Memory layers: [10, 20]
  num_memory_layers: 2
  first_memory_layer: 10
  memory_layer_interval: 10

# Positional encoding
position_encoding:
  # SWA uses RoPE, Memory uses NoPE
  rope_theta: 10000.0
  max_position_embeddings: 2048
