# Senri Training Configuration
# 学習ハイパーパラメータの設定

# Dataset settings
dataset:
  name: "wikitext"
  config: "wikitext-2-raw-v1"
  # Context length must exceed SWA window (1024) to train memory
  # 2048 tokens ensures memory is actively used during training
  max_length: 2048

# Training hyperparameters
training:
  num_epochs: 3
  batch_size: 1  # Reduced for 2048 token context on L4/T4 GPU
  gradient_accumulation_steps: 8  # Increased to maintain effective batch size
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"

  # Differential learning rate for memory layers
  # Set > 1.0 to train memory layers faster
  memory_layer_lr_multiplier: 1.0

# Evaluation settings
evaluation:
  eval_steps: 100
  save_steps: 500
  save_total_limit: 3
  logging_steps: 10

  # Early stopping
  # Set to 0 to disable early stopping
  early_stopping_patience: 2
  early_stopping_threshold: 0.0  # Minimum improvement to reset patience

# Memory optimization
optimization:
  gradient_checkpointing: true
  fp16: true

# Misc
misc:
  seed: 42
  dataloader_num_workers: 2
